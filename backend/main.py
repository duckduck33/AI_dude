from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import openai
import google.generativeai as genai
import os
import sys
import traceback
import logging
import json
from dotenv import load_dotenv
from prompts import AI1_SYSTEM_PROMPT, AI2_SYSTEM_PROMPT, INITIAL_GREETING_PROMPT, CONVERSATION_PROMPT, TOM_SYSTEM_PROMPT
from keyword_compression import KeywordCompressor
from conversation_logic import conversation_logic

# ë¡œê¹… ì„¤ì • - ëª¨ë“  ë¡œê·¸ë¥¼ ì½˜ì†”ì— ì¶œë ¥
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ì„œë²„ ì‹œì‘ ë¡œê·¸
logger.info("=== AI Chat Server Starting ===")
logger.info(f"Current working directory: {os.getcwd()}")
logger.info(f"Python version: {sys.version}")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    load_dotenv()
    logger.info("Environment variables loaded successfully")
except Exception as e:
    logger.error(f"Error loading environment variables: {e}")

# ê¸°ë³¸ OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ)
default_openai_api_key = os.getenv("OPENAI_API_KEY")
if default_openai_api_key:
    logger.info("Default OpenAI API key found in environment")
else:
    logger.warning("No default OpenAI API key found in environment")

# Gemini API ì´ˆê¸°í™”
gemini_api_key = os.getenv("GEMINI_API_KEY")
if gemini_api_key:
    genai.configure(api_key=gemini_api_key)
    gemini_model = genai.GenerativeModel('gemini-1.5-flash')
    logger.info("Gemini API configured successfully")
else:
    logger.warning("No Gemini API key found in environment")
    gemini_model = None

# ì‹œì²­ê¸°ë¡ ë°ì´í„° ë¡œë“œ
def load_viewing_history():
    try:
        with open('data/viewing_history.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            logger.info("Viewing history loaded successfully")
            return data
    except Exception as e:
        logger.error(f"Error loading viewing history: {e}")
        return None

viewing_history_data = load_viewing_history()

# í†µí•© ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
conversation_history = {
    "full_conversation": []  # ì „ì²´ ëŒ€í™”ë¥¼ ìˆœì„œëŒ€ë¡œ ì €ì¥
}

# ì••ì¶• ì‹œìŠ¤í…œ ì´ˆê¸°í™”
keyword_compressor = KeywordCompressor()

def add_to_history(speaker: str, message: str, user_message: str = None):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€ (ìˆœì„œëŒ€ë¡œ)"""
    if user_message:
        conversation_history["full_conversation"].append({
            "role": "user",
            "content": user_message
        })
    
    conversation_history["full_conversation"].append({
        "role": "assistant",
        "speaker": speaker,
        "content": message
    })

def get_recent_context(max_messages: int = 50):
    """ìµœê·¼ ëŒ€í™” ë§¥ë½ ê°€ì ¸ì˜¤ê¸° (ì „ì²´ ëŒ€í™”) - 30ë¶„ ëŒ€í™” ì§€ì›"""
    return conversation_history["full_conversation"][-max_messages:] if conversation_history["full_conversation"] else []

def clear_history():
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
    conversation_history["full_conversation"].clear()

def compress_history():
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ìŠ¤ë§ˆíŠ¸ ì••ì¶• (ì¤‘ìš”í•œ ëŒ€í™”ëŠ” ìœ ì§€)"""
    if len(conversation_history["full_conversation"]) > 100:
        # ìµœê·¼ 30ê°œëŠ” ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½
        recent_30 = conversation_history["full_conversation"][-30:]
        older_messages = conversation_history["full_conversation"][:-30]
        
        # ìš”ì•½ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
        summary = f"ì´ì „ ëŒ€í™” ìš”ì•½: {len(older_messages)}ê°œì˜ ë©”ì‹œì§€ê°€ ìˆì—ˆìŠµë‹ˆë‹¤."
        
        conversation_history["full_conversation"] = [
            {"role": "system", "content": summary}
        ] + recent_30

app = FastAPI(title="AI Chat Server", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
logger.info("CORS middleware configured")

class ChatRequest(BaseModel):
    message: str
    api_key: str = None

class InitialGreetingRequest(BaseModel):
    api_key: str = None

@app.get("/")
async def root():
    logger.info("=== Root endpoint called ===")
    return {"message": "AI Chat Server", "status": "running"}

@app.get("/health")
async def health_check():
    logger.info("=== Health check endpoint called ===")
    return {"status": "healthy", "default_openai_key_set": bool(default_openai_api_key)}

@app.get("/test")
async def test():
    logger.info("=== Test endpoint called ===")
    return {"message": "Test endpoint working!"}

@app.get("/viewing-history")
async def get_viewing_history():
    logger.info("=== Viewing history endpoint called ===")
    if viewing_history_data:
        return viewing_history_data
    else:
        return {"error": "Viewing history not available"}

@app.post("/clear-conversation")
async def clear_conversation():
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
    logger.info("=== Clear conversation endpoint called ===")
    clear_history()
    return {"message": "ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.get("/conversation-history")
async def get_conversation_history():
    """í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    logger.info("=== Conversation history endpoint called ===")
    return {
        "full_conversation": conversation_history["full_conversation"]
    }

@app.post("/initial-greeting")
async def initial_greeting(request: InitialGreetingRequest):
    logger.info("=== Initial greeting endpoint called ===")
    
    # API í‚¤ ê²°ì •
    api_key_to_use = request.api_key if request.api_key else default_openai_api_key
    
    try:
        if not api_key_to_use:
            logger.error("No OpenAI API key available")
            return {"response": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        logger.info("Setting OpenAI API key...")
        openai.api_key = api_key_to_use
        
        # ì‹œì²­ê¸°ë¡ ê¸°ë°˜ ì²« ì¸ì‚¬ ìƒì„± (AI1ì´ ë‹´ë‹¹)
        if viewing_history_data:
            top_interests = viewing_history_data.get('top_interests', [])
            favorite_category = viewing_history_data.get('favorite_category', '')
            recent_videos = viewing_history_data.get('viewing_history', [])[:3]
            
            viewing_history_info = f"""
- ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(top_interests)}
- ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {favorite_category}
- ìµœê·¼ ì‹œì²­ ì˜ìƒ: {', '.join([video['title'] for video in recent_videos])}
            """
            
            system_prompt = INITIAL_GREETING_PROMPT.format(viewing_history_info=viewing_history_info)
        else:
            system_prompt = "ë‹¹ì‹ ì€ AI DUDEì˜ ëŒ€í™” ì£¼ë„ì Jinnyì…ë‹ˆë‹¤. (ì—¬ì„±) ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ì‚¬í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ë©”ì‹œì§€ ì•ì— 'ğŸ‘© Jinny:'ë¥¼ ë¶™ì—¬ì„œ í™”ìë¥¼ ëª…ì‹œí•˜ì„¸ìš”. ì ˆëŒ€ 'AI1:'ì´ë‚˜ ë‹¤ë¥¸ ì´ë¦„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
        
        logger.info("Calling OpenAI API for initial greeting...")
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
            ]
        )
        
        ai_response = response.choices[0].message.content
        logger.info(f"OpenAI initial greeting: {ai_response}")
        return {"response": ai_response}
        
    except Exception as e:
        logger.error(f"=== Error in initial greeting endpoint ===")
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error message: {str(e)}")
        logger.error(f"Full traceback:")
        logger.error(traceback.format_exc())
        
        return {"response": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI DUDEì…ë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}

@app.post("/initial-greeting-2person")
async def initial_greeting_2person(request: InitialGreetingRequest):
    """2ì¸ ê´€ì‹¬ì‚¬ ê¸°ë°˜ ëŒ€í™” ì´ˆê¸° ì¸ì‚¬"""
    logger.info("=== 2-person initial greeting endpoint called ===")
    
    try:
        if not gemini_model:
            logger.error("No Gemini API available")
            return {"response": "Gemini APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # ì‹œì²­ê¸°ë¡ ê¸°ë°˜ ì²« ì¸ì‚¬ ìƒì„±
        if viewing_history_data:
            top_interests = viewing_history_data.get('top_interests', [])
            favorite_category = viewing_history_data.get('favorite_category', '')
            recent_videos = viewing_history_data.get('viewing_history', [])[:3]
            
            viewing_history_info = f"""
- ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(top_interests)}
- ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {favorite_category}
- ìµœê·¼ ì‹œì²­ ì˜ìƒ: {', '.join([video['title'] for video in recent_videos])}
            """
            
            system_prompt = f"""ë‹¹ì‹ ì€ AI DUDEì˜ ì¹œê·¼í•œ ì˜ì–´ ëŒ€í™” íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ìœ íŠœë¸Œ ì‹œì²­ê¸°ë¡ì„ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¸ì‚¬í•´ì£¼ì„¸ìš”.

**ì‹œì²­ê¸°ë¡ ì •ë³´:**
{viewing_history_info}

**ì²« ì¸ì‚¬ ìš”êµ¬ì‚¬í•­:**
1. ì‚¬ìš©ìì—ê²Œ ì¹œê·¼í•˜ê²Œ ì¸ì‚¬í•´ì£¼ì„¸ìš”
2. êµ¬ì²´ì ì¸ ì½˜í…ì¸ (ì˜ˆ: "ë‚˜ëŠ”ì†”ë¡œ")ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”
3. ëª¨ë“  ëŒ€í™”ëŠ” ì˜ì–´ë¡œ ì§„í–‰í•˜ë˜, í•œêµ­ì–´ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”
4. ì¹œê·¼í•˜ê³  í˜¸ê¸°ì‹¬ ë§ì€ í†¤ìœ¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”
5. ë©”ì‹œì§€ ì•ì— "ğŸ¤– AI:"ë¥¼ ë¶™ì—¬ì„œ í™”ìë¥¼ ëª…ì‹œí•˜ì„¸ìš”

**ì˜ˆì‹œ:**
"ğŸ¤– AI: Hello! I'm your AI conversation partner! (ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë‹¹ì‹ ì˜ AI ëŒ€í™” íŒŒíŠ¸ë„ˆì˜ˆìš”!) I noticed you love watching dating shows like 'I'm Solo'! (ë‹¹ì‹ ì´ 'ë‚˜ëŠ”ì†”ë¡œ' ê°™ì€ ì—°ì•  í”„ë¡œê·¸ë¨ì„ ì¢‹ì•„í•œë‹¤ëŠ” ê±¸ ì•Œì•„ëƒˆì–´ìš”!) Which couple impressed you the most? (ì–´ë–¤ ì»¤í”Œì´ ê°€ì¥ ì¸ìƒì ì´ì—ˆë‚˜ìš”?)"
"""
        else:
            system_prompt = "ë‹¹ì‹ ì€ AI DUDEì˜ ì¹œê·¼í•œ ì˜ì–´ ëŒ€í™” íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ì‚¬í•´ì£¼ì„¸ìš”. ë©”ì‹œì§€ ì•ì— 'ğŸ¤– AI:'ë¥¼ ë¶™ì—¬ì„œ í™”ìë¥¼ ëª…ì‹œí•˜ì„¸ìš”."
        
        logger.info("Calling Gemini API for 2-person initial greeting...")
        response = gemini_model.generate_content(
            f"{system_prompt}\n\nì‚¬ìš©ì: ì•ˆë…•í•˜ì„¸ìš”\n\nAI:"
        )
        
        ai_response = response.text
        logger.info(f"Gemini 2-person initial greeting: {ai_response}")
        return {"response": ai_response}
        
    except Exception as e:
        logger.error(f"=== Error in 2-person initial greeting endpoint ===")
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error message: {str(e)}")
        logger.error(f"Full traceback:")
        logger.error(traceback.format_exc())
        
        return {"response": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI DUDEì…ë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}

@app.post("/chat")
async def chat(request: ChatRequest):
    logger.info(f"=== Chat endpoint called ===")
    logger.info(f"Received message: {request.message}")
    
    # API í‚¤ ê²°ì • (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë°›ì€ í‚¤ ìš°ì„ , ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜)
    api_key_to_use = request.api_key if request.api_key else default_openai_api_key
    
    try:
        if not api_key_to_use:
            logger.error("No OpenAI API key available")
            return {"response": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        
        if not gemini_model:
            logger.error("No Gemini API available")
            return {"response": "Gemini APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        logger.info("Setting OpenAI API key...")
        openai.api_key = api_key_to_use
        
        # ì‹œì²­ê¸°ë¡ ì •ë³´ ì¤€ë¹„
        if viewing_history_data:
            top_interests = viewing_history_data.get('top_interests', [])
            favorite_category = viewing_history_data.get('favorite_category', '')
            recent_videos = viewing_history_data.get('viewing_history', [])[:3]
            
            viewing_history_info = f"""
- ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(top_interests)}
- ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {favorite_category}
- ìµœê·¼ ì‹œì²­ ì˜ìƒ: {', '.join([video['title'] for video in recent_videos])}
            """
        else:
            viewing_history_info = "ì‹œì²­ê¸°ë¡ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # Jinny (OpenAI) ë¨¼ì € ì‘ë‹µ
        logger.info("Calling OpenAI API for Jinny...")
        jinny_system_prompt = f"""{AI1_SYSTEM_PROMPT}

ì‹œì²­ê¸°ë¡ ì •ë³´:
{viewing_history_info}

Jinnyê°€ ì‚¬ìš©ìì—ê²Œë§Œ ì‘ë‹µí•˜ì„¸ìš”. Tomì—ê²Œ ë§ì„ ê±¸ì§€ ë§ˆì„¸ìš”. ë©”ì‹œì§€ ì•ì— "ğŸ‘© Jinny:"ë¥¼ ë¶™ì—¬ì„œ í™”ìë¥¼ ëª…ì‹œí•˜ì„¸ìš”."""
        
        # Jinny ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„
        jinny_messages = [{"role": "system", "content": jinny_system_prompt}]
        
        # ì••ì¶•ëœ ëŒ€í™” ë§¥ë½ ì‚¬ìš©
        compressed_data = keyword_compressor.compress_conversation(
            conversation_history["full_conversation"], 
            keep_recent=8
        )
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì••ì¶•ëœ ë§¥ë½
        jinny_messages = [{"role": "system", "content": jinny_system_prompt}]
        
        # ì••ì¶•ëœ ë§¥ë½ ì¶”ê°€
        if compressed_data["compressed_context"] != "ëŒ€í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.":
            jinny_messages.append({
                "role": "system", 
                "content": f"ì´ì „ ëŒ€í™” ë§¥ë½: {compressed_data['compressed_context']}"
            })
        
        # ìµœê·¼ ë©”ì‹œì§€ ì¶”ê°€
        for msg in compressed_data["recent_messages"]:
            if msg["role"] == "user":
                jinny_messages.append({"role": "user", "content": msg["content"]})
            elif msg["role"] == "assistant":
                jinny_messages.append({"role": "assistant", "content": msg["content"]})
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        jinny_messages.append({"role": "user", "content": request.message})
        
        jinny_response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=jinny_messages
        )
        
        jinny_message = jinny_response.choices[0].message.content
        logger.info(f"Jinny response: {jinny_message}")
        
        # Tom (Gemini) ë…ë¦½ì  ì‘ë‹µ
        logger.info("Calling Gemini API for Tom...")
        tom_system_prompt = TOM_SYSTEM_PROMPT.format(viewing_history_info=viewing_history_info)
        
        # Tom ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„ (ì „ì²´ ëŒ€í™” ë§¥ë½)
        recent_context = get_recent_context(50)
        tom_context_text = ""
        if recent_context:
            context_messages = []
            for ctx in recent_context:
                if ctx["role"] == "user":
                    context_messages.append(f"ì‚¬ìš©ì: {ctx['content']}")
                elif ctx["role"] == "assistant":
                    speaker = ctx.get("speaker", "AI")
                    context_messages.append(f"{speaker}: {ctx['content']}")
            tom_context_text = "\n\nìµœê·¼ ëŒ€í™” ë§¥ë½:\n" + "\n".join(context_messages)
        
        tom_response = gemini_model.generate_content(
            f"{tom_system_prompt}{tom_context_text}\n\nì‚¬ìš©ì ë©”ì‹œì§€: {request.message}\n\nTomì´ ì‚¬ìš©ìì—ê²Œ ë…ë¦½ì ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”. Jinnyì˜ ì‘ë‹µì„ ì°¸ê³ í•˜ë˜, ë³„ë„ì˜ ë©”ì‹œì§€ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë©”ì‹œì§€ ì•ì— 'ğŸ‘¨ Tom:'ì„ ë¶™ì—¬ì„œ í™”ìë¥¼ ëª…ì‹œí•˜ì„¸ìš”."
        )
        
        tom_message = tom_response.text
        logger.info(f"Tom response: {tom_message}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥
        add_to_history("jinny", jinny_message, request.message)
        add_to_history("tom", tom_message, request.message)
        
        # íˆìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì••ì¶•
        compress_history()
        
        # ì´ë¦„ í˜¸ì¶œ ê°ì§€
        name_detection = conversation_logic.detect_name_call(request.message)
        
        # ëŒ€í™” ë¡œì§ì„ ì‚¬ìš©í•´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
        conversation_logic.update_conversation_state(request.message)
        
        # ì´ë¦„ì´ í˜¸ì¶œë˜ì—ˆìœ¼ë©´ í•´ë‹¹ AIë§Œ ì‘ë‹µ
        if name_detection["is_direct_call"]:
            called_names = name_detection["called_names"]
            if "jinny" in called_names:
                combined_response = jinny_message
                logger.info(f"Jinny called directly: {combined_response}")
            elif "tom" in called_names:
                combined_response = tom_message
                logger.info(f"Tom called directly: {combined_response}")
            else:
                combined_response = conversation_logic.create_response(
                    request.message, jinny_message, tom_message
                )
        else:
            combined_response = conversation_logic.create_response(
                request.message, jinny_message, tom_message
            )
        
        logger.info(f"Response created: {combined_response}")
        return {"response": combined_response}
        
    except Exception as e:
        logger.error(f"=== Error in chat endpoint ===")
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error message: {str(e)}")
        logger.error(f"Full traceback:")
        logger.error(traceback.format_exc())
        
        # êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        if "api_key" in str(e).lower():
            return {"response": "OpenAI API í‚¤ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        elif "rate_limit" in str(e).lower():
            return {"response": "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}
        else:
            return {"response": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

@app.post("/chat-2person")
async def chat_2person(request: ChatRequest):
    """2ì¸ ê´€ì‹¬ì‚¬ ê¸°ë°˜ ëŒ€í™” (Gemini AIë§Œ ì‚¬ìš©)"""
    logger.info(f"=== 2-person chat endpoint called ===")
    logger.info(f"Received message: {request.message}")
    
    try:
        if not gemini_model:
            logger.error("No Gemini API available")
            return {"response": "Gemini APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # ì‹œì²­ê¸°ë¡ ì •ë³´ ì¤€ë¹„
        if viewing_history_data:
            top_interests = viewing_history_data.get('top_interests', [])
            favorite_category = viewing_history_data.get('favorite_category', '')
            recent_videos = viewing_history_data.get('viewing_history', [])[:3]
            
            viewing_history_info = f"""
- ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(top_interests)}
- ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {favorite_category}
- ìµœê·¼ ì‹œì²­ ì˜ìƒ: {', '.join([video['title'] for video in recent_videos])}
            """
        else:
            viewing_history_info = "ì‹œì²­ê¸°ë¡ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # Gemini AI ì „ìš© í”„ë¡¬í”„íŠ¸
        gemini_system_prompt = f"""ë‹¹ì‹ ì€ AI DUDEì˜ ì¹œê·¼í•œ ì˜ì–´ ëŒ€í™” íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤.

**ì—­í• ê³¼ ì±…ì„:**
1. ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
2. ëª¨ë“  ëŒ€í™”ëŠ” ì˜ì–´ë¡œ ì§„í–‰í•˜ë˜, í•œêµ­ì–´ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”
3. ì‚¬ìš©ìê°€ ì–´ë ¤ì›Œí•  ë•Œ ì¦‰ì‹œ ë„ì›€ì„ ì œê³µí•˜ì„¸ìš”
4. ì˜ì–´ í•™ìŠµì— ëŒ€í•œ ê¸ì •ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”

**ëŒ€í™” ìŠ¤íƒ€ì¼:**
- ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤
- ì‚¬ìš©ìì˜ ê°ì •ì„ ê³µê°í•˜ê³  ì§€ì§€
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ì—°ê²°
- ë©”ì‹œì§€ ì•ì— "ğŸ¤– AI:"ë¥¼ ë¶™ì—¬ì„œ í™”ìë¥¼ ëª…ì‹œ

**ì‹œì²­ê¸°ë¡ ì •ë³´:**
{viewing_history_info}

**ëŒ€í™” ê·œì¹™:**
1. ëª¨ë“  ëŒ€í™”ëŠ” ì˜ì–´ë¡œ ì§„í–‰í•˜ë˜, ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ í•œêµ­ì–´ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”
2. ì‹œì²­ê¸°ë¡ì˜ ê´€ì‹¬ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ì„¸ìš”
3. ì˜ì–´ í‘œí˜„ì„ ì‚¬ìš©í•  ë•Œë§ˆë‹¤ í•œêµ­ì–´ë¡œ ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”
4. ë©”ì‹œì§€ ì•ì— "ğŸ¤– AI:"ë¥¼ ë¶™ì—¬ì„œ í™”ìë¥¼ ëª…ì‹œí•˜ì„¸ìš”
5. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”
"""
        
        # Gemini AI ì‘ë‹µ
        logger.info("Calling Gemini API for 2-person chat...")
        
        # ì••ì¶•ëœ ëŒ€í™” ë§¥ë½ ì‚¬ìš©
        compressed_data = keyword_compressor.compress_conversation(
            conversation_history["full_conversation"], 
            keep_recent=8
        )
        
        # ì••ì¶•ëœ ë§¥ë½ ì¶”ê°€
        context_text = ""
        if compressed_data["compressed_context"] != "ëŒ€í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.":
            context_text = f"\n\nì´ì „ ëŒ€í™” ë§¥ë½: {compressed_data['compressed_context']}"
        
        # ìµœê·¼ ë©”ì‹œì§€ ì¶”ê°€
        recent_text = ""
        for msg in compressed_data["recent_messages"]:
            if msg["role"] == "user":
                recent_text += f"ì‚¬ìš©ì: {msg['content']}\n"
            elif msg["role"] == "assistant":
                recent_text += f"AI: {msg['content']}\n"
        
        gemini_response = gemini_model.generate_content(
            f"{gemini_system_prompt}{context_text}\n\nìµœê·¼ ëŒ€í™”:\n{recent_text}\n\nì‚¬ìš©ì: {request.message}\n\nAI:"
        )
        
        ai_message = gemini_response.text
        logger.info(f"Gemini 2-person response: {ai_message}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥
        add_to_history("ai", ai_message, request.message)
        compress_history()
        
        return {"response": ai_message}
        
    except Exception as e:
        logger.error(f"=== Error in 2-person chat endpoint ===")
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error message: {str(e)}")
        logger.error(f"Full traceback:")
        logger.error(traceback.format_exc())
        
        return {"response": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

# ì„œë²„ ì‹œì‘ ì‹œ ë¡œê·¸
logger.info("=== AI Chat Server initialized successfully ===")
logger.info("Available endpoints:")
logger.info("  - GET /")
logger.info("  - GET /health")
logger.info("  - GET /test")
logger.info("  - GET /viewing-history")
logger.info("  - POST /initial-greeting")
logger.info("  - POST /initial-greeting-2person")
logger.info("  - POST /chat")
logger.info("  - POST /chat-2person")
logger.info("  - GET /docs (FastAPI documentation)")
